---
title: Eluder Dimension and Potential Lemma
speaker:
  name: Yingru Li
  affil: CUHK-SZ
  url: 
--- 

[Slides](/static/files/SP21-Slides/RL-Theory-2021-03-12-Eluder.pdf)

Speaker: Yingru Li

Title: Eluder dimension and potential lemma

Short Abstract:
The first establishes a connection between posterior sampling and UCB algorithms. This result lets us convert regret bounds developed for UCB algorithms into Bayesian regret bounds for posterior sampling.

Second theoretical contribution is a Bayesian regret bound for posterior sampling that applies broadly on general function classes and can be specialised to many model classes.
This bound depends on a new notion called the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm Bayesian regret bounds for specific model classes, the general bound matches the best available for linear models and is stronger than the best available for generalized linear models.

Finally, if time permitted, I would like to discuss the conceptual relationship between eluder dimension and celebrated elliptical potential lemma from both linear algebra viewpoint and information theory viewpoint.

Primary Reference:
Russo, Daniel, and Benjamin Van Roy. "Learning to optimize via posterior sampling." Mathematics of Operations Research 39.4 (2014): 1221-1243.

Russo, Daniel, and Benjamin Van Roy. "Eluder Dimension and the Sample Complexity of Optimistic Exploration." NIPS. 2013.

<!-- Supplementary Reference:
Lu, Xiuyuan, and Benjamin Van Roy. "Information-theoretic confidence bounds for reinforcement learning." arXiv preprint arXiv:1911.09724 (2019).
Carpentier, Alexandra, Claire Vernade, and Yasin Abbasi-Yadkori. "The Elliptical Potential Lemma Revisited." arXiv preprint arXiv:2010.10182 (2020).
Hamidi, Nima, and Mohsen Bayati. "The Randomized Elliptical Potential Lemma with an Application to Linear Thompson Sampling." arXiv preprint arXiv:2102.07987 (2021). -->